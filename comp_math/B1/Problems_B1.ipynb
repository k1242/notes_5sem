{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MeXTSdSfKdL2"
   },
   "source": [
    "## *Задача 1* \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19smb2owQS40"
   },
   "source": [
    "Реализовать генератор матрциц, который должен поддерживать функции:\n",
    "* Генерация абсолютно случайной матрицы $n\\times m$\n",
    "* Генерация случайной диагональной матрицы $n\\times n$\n",
    "* Генерация случайной верхнетреугольной матрицы\n",
    "* Генерация случайной нижнетреугольной матрицы\n",
    "* Генерация симметричной матрицы\n",
    "* Генерация вырожденной матрицы\n",
    "* Генерация матрицы ступенчатого вида $n\\times n$ ранга $m$\n",
    "* Генерация возмущения матрицы $n\\times m$, каждый элемент которой не превосходит по модулю заданный $\\varepsilon$\n",
    "\n",
    "Оценить вероятность того, что созданная матрица будет вырожденной. \n",
    "\n",
    "Оценить величину нормы матрицы возмущений в зависимости от параметра $\\varepsilon$ (оценить верхную границу).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sS3EHa1aKmJi"
   },
   "source": [
    "# Задача 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nMUUtmljQZO1"
   },
   "source": [
    "Используя ряд Маклорена, реализовать вычисление основных элементарных функций: \n",
    "* Экспонента\n",
    "* Натуральный логарифм\n",
    "* Синус\n",
    "* Косинус\n",
    "* Тангенс\n",
    "* Котангенс\n",
    "* Арксинус\n",
    "* Арккосинус\n",
    "* Арктангенс\n",
    "* Гиперболический синус \n",
    "* Гиперболический косинус\n",
    "* Гиперболический тангенс\n",
    "* Гиперболический арктангенс\n",
    "\n",
    "Оценить величину машинного эпсилон. Предложить модификации для некоторых функций и сравнить полученные результаты."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpR6GBRtKmXq"
   },
   "source": [
    "# Задача 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xQ7BfspuTsOt"
   },
   "source": [
    "Реализовать вычисление трех основных норм векторов (L1, L2 и кубическую) и подчиненных им матричных норм. Реализовать вычисление числа обусловленности.\n",
    "\n",
    "Примечание: для вычисления собственных значений можно использовать linalg.eigvals из модуля scipy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8-476ZEtKmk5"
   },
   "source": [
    "# Задача 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xzG3RPf-T07c"
   },
   "source": [
    "Реализовать метод Гаусса приведения матрицы к ступенчатому виду. Реализовать функцию вычисления ранга матрицы. Сгенерировать вырожденные матрицы различных рангов и размеров и проверить алгоритм."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s8jGlqhJKmpB"
   },
   "source": [
    "# Задача 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "smdRrcYuT4he"
   },
   "source": [
    "Реализовать метод Гаусса решения СЛАУ. Использовать данный метод для решения систем различных размеров. Оценить скорость работы метода Гаусса (необходимое количество операций) в зависимости от размера системы. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "634QUTkyKnWR"
   },
   "source": [
    "# Задача 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cxldprgmTnF1"
   },
   "source": [
    "Сгенерировать СЛАУ (размер матрицы должен быть не менее $50\\times 50$). Решить СЛАУ методом Гаусса для различных возмущений столбца свободных членов. Оценить число обусловленности, используя полученные результаты. Вычислить число обусловленности и сравнить с численными оценками."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KQaBP8lURmuE"
   },
   "source": [
    "# Дополнительные задачи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "igJf5dJZRoXN"
   },
   "source": [
    "## *Задача 7* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "elhxJ6fJRRq3"
   },
   "source": [
    "В этой задаче требуется найти аналитическое решение и проверить его с \n",
    "\n",
    "помощью вычислений на Python. Решить только один пример (на выбор).\n",
    "\n",
    "Примеры решения подобных задач есть в документе \"Визуализация данных\" к занятию А1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zh4lVDmtIaCV"
   },
   "source": [
    "1.1. Чему равна погрешность в определении действительного корня $x=1$ уравнения $a x^{4}+b x^{3}+d x+e=0$, если $a=1 \\pm 10^{-3}, b=1 \\pm 10^{-3}$, $d=-1 \\pm 10^{-3}, e=-1 \\pm 10^{-3}$ ?\n",
    "\n",
    "1.2. Чему равна погрешность в определении корней уравнения $a x^{3}+b x^{2}=0$, если $a=1 \\pm 10^{-3}, b=-4 \\pm 10^{-3}$ ?\n",
    "\n",
    "1.3. С каким числом верных знаков (или относительной погрешностью) должен быть известен свободный член в уравнении $x^{2}-2 x+0.999993751=0$, чтобы корни имели четыре верных знака?\n",
    "\n",
    "1.4. С каким числом верных знаков (или относительной погрешностью) должен быть известен свободный член в уравнении $x^{2}-4 x+3.999901=0$, чтобы корни имели четыре верных знака?\n",
    "\n",
    "1.5. Определить оптимальный шаг $h=$ const формулы численного дифференцирования $f^{\\prime}(x-h) \\approx(f(x)-f(x-h)) / h, \\max _{[x-h, x]}\\left|f^{\\prime \\prime}(x)\\right| \\leq 100$, если абсолютная погрешность при задании $f(x), f(x-h)$ не превосходит $\\Delta=0.1$\n",
    "\n",
    "1.6. Определить оптимальный шаг $h=$ const формулы численного дифференцирования $f^{\\prime}(x) \\approx(f(x+h)-f(x-h)) / 2 h, \\max _{[x-h, x+h]}\\left|f^{\\prime \\prime \\prime}(x)\\right| \\leq 100$, если абсолютная погрешность при задании, $f(x \\pm h)$ не превосходит $\\Delta=0.1$.\n",
    "\n",
    "1.7. Определить оптимальный шаг $h=$ const формулы численного дифференцирования $f^{\\prime}(x) \\approx(3 f(x)-4 f(x-h)+f(x-2 h)) / 2 h$, $\\max _{[x-2 h, x]}\\left|f^{\\prime \\prime \\prime}(x)\\right| \\leq 100$, если абсолютная погрешность при задании $f(x)$, $f(x-h), f(x-2 h)$ не превосходит $\\Delta=0.1$.\n",
    "\n",
    "1.8. Пусть приближенное значение первой производной функции $f(x)$ определяется при $h \\ll 1$ по формуле\n",
    "$f^{\\prime}(x) \\approx(3 f(x)-4 f(x-h)+f(x-2 h)) / 2 h$, а сами значения $f(x), f(x-h), f(x-2 h)$ вычисляются с абсолютной погрешностью $\\Delta$.\n",
    "Какую погрешность можно ожидать при вычислении производной, если $\\left|f^{(k)}(x)\\right| \\leq M_{k}, k=1,2, \\ldots ?$\n",
    "\n",
    "1.9. Пусть задана последовательность чисел $x_{n}, n=0,1,2, \\ldots$, причем $x_{n+1}-5 x_{n}=4$, а $x_{0}$ известно с относительной погрешностью $10^{-6} .$ При каких значениях $x_{0}$ относительная погрешность при вычислении $x_{n}$ будет быстро возрастать с ростом $n$ ?\n",
    "\n",
    "1.10. Пусть задана последовательность чисел $x_{n}, n=0,1,2, \\ldots$, причем $5 x_{n+1}-x_{n}=4$, а $x_{0}$ известно с относительной погрешностью $10^{-6} .$ При каких значениях $x_{0}$ относительная погрешность при вычислении $x_{n}$ будет быстро возрастать с ростом $n$ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WfmTt-36Te8Q"
   },
   "source": [
    "## *Задача 8* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8gHzjikte83x"
   },
   "source": [
    "Выбор метрики (нормы разницы между любыми двумя векторами, или функции расстояния между любой парой точек) очень важен для многих алгоритмов машинного обучения. Рассмотрим на примере задачи кластеризации. \n",
    "\n",
    "Кластеризация — это разделение множества входных векторов на группы (кластеры) по степени «схожести» друг с другом.\n",
    "\n",
    "Кластеризация в Data Mining приобретает ценность тогда, когда она выступает одним из этапов анализа данных, построения законченного аналитического решения. Аналитику часто легче выделить группы схожих объектов, изучить их особенности и построить для каждой группы отдельную модель, чем создавать одну общую модель для всех данных. Таким приемом постоянно пользуются в маркетинге, выделяя группы клиентов, покупателей, товаров и разрабатывая для каждой из них отдельную стратегию.\n",
    "\n",
    "\n",
    "Евклидова метрика \n",
    "\n",
    "\n",
    "— наиболее распространенная. Она является геометрическим расстоянием в многомерном пространстве.\n",
    "\n",
    "\n",
    "Квадрат евклидовой метрики. \n",
    "\n",
    "\n",
    "Иногда может возникнуть желание возвести в квадрат стандартное евклидово расстояние, чтобы придать большие веса более отдаленным друг от друга объектам.\n",
    "\n",
    "\n",
    "Метрика городских кварталов (манхэттенская). \n",
    "\n",
    "\n",
    "Это расстояние является суммой модулей разностей координат. В большинстве случаев эта метрика приводит к таким же результатам, как и для обычного расстояния Евклида. Однако отметим, что для этой меры влияние отдельных больших разностей (выбросов) уменьшается (так как они не возводятся в квадрат).\n",
    "\n",
    "Расстояние Чебышева. \n",
    "\n",
    "Это метрика шахматной доски (Расстоянием Чебышёва между n-мерными числовыми векторами называется максимум модуля разности компонент этих векторов). Это расстояние может оказаться полезным, когда желают определить два объекта как «различные», если они различаются по какой-либо одной координате (каким-либо одним измерением).\n",
    "\n",
    "Расстояние Чебышёва называют также метрикой Чебышёва, равномерной метрикой, sup-метрикой и бокс-метрикой; также иногда она называется метрикой решётки, метрикой шахматной доски, метрикой хода короля и 8-метрикой.\n",
    "\n",
    "Степенная метрика. \n",
    "\n",
    "Иногда желают прогрессивно увеличить или уменьшить вес, относящийся к размерности, для которой соответствующие объекты сильно отличаются. Это может быть достигнуто с использованием степенного расстояния.\n",
    "\n",
    "\n",
    "Выбор метрики (критерия схожести) лежит полностью на исследователе. При выборе различных мер результаты кластеризации могут существенно отличаться.\n",
    "\n",
    "Алгоритм k-means (k-средних)\n",
    "\n",
    "Наиболее простой, но в то же время достаточно неточный метод кластеризации в классической реализации. Он разбивает множество элементов векторного пространства на заранее известное число кластеров k. Действие алгоритма таково, что он стремится минимизировать среднеквадратичное отклонение на точках каждого кластера. Основная идея заключается в том, что на каждой итерации перевычисляется центр масс для каждого кластера, полученного на предыдущем шаге, затем векторы разбиваются на кластеры вновь в соответствии с тем, какой из новых центров оказался ближе по выбранной метрике. Алгоритм завершается, когда на какой-то итерации не происходит изменения кластеров.\n",
    "\n",
    "Проблемы алгоритма k-means:\n",
    "* необходимо заранее знать количество кластеров. Мной было предложено метод определения количества кластеров, который основывался на нахождении кластеров, распределенных по некоему закону (в моем случае все сводилось к нормальному закону). После этого выполнялся классический алгоритм k-means, который давал более точные результаты.\n",
    "* алгоритм очень чувствителен к выбору начальных центров кластеров. Классический вариант подразумевает случайный выбор класторов, что очень часто являлось источником погрешности. Как вариант решения, необходимо проводить исследования объекта для более точного определения центров начальных кластеров. В моем случае на начальном этапе предлагается принимать в качестве центов самые отдаленные точки кластеров.\n",
    "* не справляется с задачей, когда объект принадлежит к разным кластерам в равной степени или не принадлежит ни одному.\n",
    "\n",
    "Нечеткий алгоритм кластеризации с-means\n",
    "\n",
    "С последней проблемой k-means успешно справляется алгоритм с-means. Вместо однозначного ответа на вопрос к какому кластеру относится объект, он определяет вероятность того, что объект принадлежит к тому или иному кластеру. Таким образом, утверждение «объект А принадлежит к кластеру 1 с вероятностью 90%, к кластеру 2 — 10% » верно и более удобно.\n",
    "\n",
    "Остальные проблемы у с-means такие же, как у k-means, но они нивелируются благодаря нечеткости разбиения.\n",
    "\n",
    "Метод нечеткой кластеризации C-средних имеет ограниченное применение из-за существенного недостатка — невозможность корректного разбиения на кластеры, в случае когда кластеры имеют различную дисперсию по различным размерностям (осям) элементов (например, кластер имеет форму эллипса). Данный недостаток устранен в алгоритмах Mixture models и GMM (Gaussian mixture models). \n",
    "\n",
    "\n",
    "Документация методов кластеризации для sklearn есть здесь https://scikit-learn.org/stable/modules/clustering.html#k-means . \n",
    "\n",
    "\n",
    "Используя библиотеку scikit-learn, реализуйте Gaussian mixture models и обычный k-means.  Подберите такой набор данных, на котором первый метод справляется хорошо, а второй метод даёт плохие результаты, и продемонстрируйте это. Сделайте это для нескольких разных метрик и сравните результаты между собой.\n",
    "\n",
    "https://scikit-learn.ru/example/  примеры подобного.\n",
    "\n",
    "https://neurohive.io/ru/osnovy-data-science/vvedenie-v-scikit-learn/  введение в sklearn. На этом сайте много полезных статей и ссылок на курсы.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "MeXTSdSfKdL2",
    "sS3EHa1aKmJi",
    "gpR6GBRtKmXq",
    "8-476ZEtKmk5",
    "s8jGlqhJKmpB",
    "634QUTkyKnWR",
    "igJf5dJZRoXN",
    "WfmTt-36Te8Q"
   ],
   "name": "Семинар B1 (1).ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
